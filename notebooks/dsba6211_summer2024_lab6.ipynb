{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neal-logan/dsba6211-summer2024/blob/main/notebooks/dsba6211_summer2024_lab6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HIuaawzR8RR"
      },
      "source": [
        "# Getting Started with Prompt Engineering\n",
        "\n",
        "This notebook contains examples and exercises to learning about prompt engineering. It was originally created by DAIR.AI | Elvis Saravia with modifications.\n",
        "\n",
        "We will be using the [OpenAI APIs](https://platform.openai.com/) for all examples. I am using the default settings `temperature=0.7` and `top-p=1`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jego_SZUR8RR"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0do4hkhR8RS"
      },
      "source": [
        "## 1. Prompt Engineering Basics: OpenAI API and configurations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOMd4TbUR8RS"
      },
      "source": [
        "Below we are loading the necessary libraries, utilities, and configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R1KWAb3_R8RS"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# update or install the necessary libraries\n",
        "!pip install --upgrade openai==1.35.1\n",
        "!pip install --upgrade langchain==0.2.5\n",
        "!pip install --upgrade langchain-openai==0.1.8\n",
        "!pip install langchain-community==0.2.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yANHPO1MR8RS"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "import IPython\n",
        "from langchain.llms import OpenAI\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YhuFzr-R8RS"
      },
      "source": [
        "Load environment variables. Since I'm running this in a Colab notebook, I'm using `userdata.get()`. Just make sure to add your API Key into your Colab.\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5wEevNCOf80GTHwptPTB4g.png)\n",
        "\n",
        "Alternatively, you can use `python-dotenv` with a `.env` file with your `OPENAI_API_KEY` then load it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7Qs7ie0kR8RS"
      },
      "outputs": [],
      "source": [
        "# API configuration\n",
        "openai.api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# for LangChain\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Types of Endpoints\n",
        "\n",
        "OpenAI (and other LLM) API's typically have two types API endpoints: completion and chat.\n",
        "\n",
        "![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccf3b37b-ad35-43b3-8cac-607a01473ba8_2719x508.png)\n",
        "\n",
        "Source: [Generally Intelligent SubStack](https://generallyintelligent.substack.com/p/chat-vs-completion-endpoints)\n",
        "\n",
        "Originally, the completions endpoint was the first endpoint. However, OpenAI has [announced](https://community.openai.com/t/completion-models-are-now-considered-legacy/656302) it is deprecating that endpoint.\n",
        "\n",
        "`/completions` endpoint provides the completion for a single prompt and takes a single string as an input, whereas the `/chat/completions` provides the responses for a given dialog and requires the input in a specific format corresponding to the message history. Instead of taking a `prompt`, Chat models take a list of `messages` as input and return a model-generated message as output.\n",
        "\n",
        "\n",
        "\n",
        "| MODEL FAMILIES               | EXAMPLES                                         | API ENDPOINT                               |\n",
        "|------------------------------|--------------------------------------------------|--------------------------------------------|\n",
        "| Newer models (2023â€“)         | gpt-4, gpt-4-turbo-preview, gpt-3.5-turbo        | https://api.openai.com/v1/chat/completions |\n",
        "| Updated legacy models (2023) | gpt-3.5-turbo-instruct, babbage-002, davinci-002 | https://api.openai.com/v1/completions      |\n",
        "\n",
        "\n",
        "Although the chat format is designed to make multi-turn conversations easy, itâ€™s just as useful for single-turn tasks without any conversation.\n",
        "\n",
        "We're going to the use the Chat `openai.chat.completions` endpoint.\n",
        "\n",
        "For more details, check out [OpenAI's docs](https://platform.openai.com/docs/guides/text-generation/chat-completions-vs-completions)."
      ],
      "metadata": {
        "id": "nAbquZOc9hjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
        "  ]\n",
        ")\n",
        "\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKY-ePiJC3y6",
        "outputId": "870d44ed-db78-400d-d6e3-f2cf84abd8cb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-9ihDueluA80QYrwMj7Le43meTSOKK', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The 2020 World Series was played at Globe Life Field in Arlington, Texas.', role='assistant', function_call=None, tool_calls=None))], created=1720439610, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=17, prompt_tokens=53, total_tokens=70))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Typically, what we're most interested in is the `choices[0].message.content`:"
      ],
      "metadata": {
        "id": "jcUqxHrqDVeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "TrAlcD1KEJ0X",
        "outputId": "64aae9f7-9e6a-44e8-e05d-6245c8d3e4c9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The 2020 World Series was played at Globe Life Field in Arlington, Texas.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make things a bit easier to modify parameters, we can generalize the calls as a function, including parameters and messages."
      ],
      "metadata": {
        "id": "_JHq92uCDF38"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "S9HGq2JQR8RS"
      },
      "outputs": [],
      "source": [
        "def get_completion(params, messages):\n",
        "    \"\"\" GET completion from openai api\"\"\"\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "        model = params['model'],\n",
        "        messages = messages,\n",
        "        temperature = params['temperature'],\n",
        "        max_tokens = params['max_tokens'],\n",
        "        top_p = params['top_p'],\n",
        "        frequency_penalty = params['frequency_penalty'],\n",
        "        presence_penalty = params['presence_penalty'],\n",
        "    )\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's a variety of different parameters that are common with LLM's. Since LLM's are really just word predictors (auto-complete, distributions over vocabulary), they require [different sampling methods](https://huyenchip.com/2024/01/16/sampling.html) to get the next word.\n",
        "\n",
        "| Parameter          | Description                                                                                                                                                   |\n",
        "|--------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| `model`            | Specifies the model to be used for generating responses. Different models may have different capabilities, size, and performance characteristics.             |\n",
        "\n",
        "Here's [an outline](https://www.promptingguide.ai/introduction/settings) of different common LLM parameters.\n"
      ],
      "metadata": {
        "id": "rqY79t7q8SFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!openai --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZ24btuoHvu9",
        "outputId": "0d54cfe6-6414-48d1-80ec-8e859ec9ce1a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: openai [-h] [-v] [-b API_BASE] [-k API_KEY] [-p PROXY [PROXY ...]] [-o ORGANIZATION]\n",
            "              [-t {openai,azure}] [--api-version API_VERSION] [--azure-endpoint AZURE_ENDPOINT]\n",
            "              [--azure-ad-token AZURE_AD_TOKEN] [-V]\n",
            "              {api,tools,migrate,grit} ...\n",
            "\n",
            "positional arguments:\n",
            "  {api,tools,migrate,grit}\n",
            "    api                 Direct API calls\n",
            "    tools               Client side tools for convenience\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  -v, --verbose         Set verbosity.\n",
            "  -b API_BASE, --api-base API_BASE\n",
            "                        What API base url to use.\n",
            "  -k API_KEY, --api-key API_KEY\n",
            "                        What API key to use.\n",
            "  -p PROXY [PROXY ...], --proxy PROXY [PROXY ...]\n",
            "                        What proxy to use.\n",
            "  -o ORGANIZATION, --organization ORGANIZATION\n",
            "                        Which organization to run as (will use your default organization if not\n",
            "                        specified)\n",
            "  -t {openai,azure}, --api-type {openai,azure}\n",
            "                        The backend API to call, must be `openai` or `azure`\n",
            "  --api-version API_VERSION\n",
            "                        The Azure API version, e.g. 'https://learn.microsoft.com/en-us/azure/ai-\n",
            "                        services/openai/reference#rest-api-versioning'\n",
            "  --azure-endpoint AZURE_ENDPOINT\n",
            "                        The Azure endpoint, e.g. 'https://endpoint.openai.azure.com'\n",
            "  --azure-ad-token AZURE_AD_TOKEN\n",
            "                        A token from Azure Active Directory, https://www.microsoft.com/en-\n",
            "                        us/security/business/identity-access/microsoft-entra-id\n",
            "  -V, --version         show program's version number and exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_open_params(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=256,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0,\n",
        "):\n",
        "    \"\"\" set openai parameters\"\"\"\n",
        "\n",
        "    openai_params = {}\n",
        "\n",
        "    openai_params['model'] = model\n",
        "    openai_params['temperature'] = temperature\n",
        "    openai_params['max_tokens'] = max_tokens\n",
        "    openai_params['top_p'] = top_p\n",
        "    openai_params['frequency_penalty'] = frequency_penalty\n",
        "    openai_params['presence_penalty'] = presence_penalty\n",
        "    return openai_params"
      ],
      "metadata": {
        "id": "6EWfKXQfBgm8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8Ljy2AtbR8RT"
      },
      "outputs": [],
      "source": [
        "# basic example\n",
        "\n",
        "params = set_open_params(temperature = 0.7)\n",
        "\n",
        "prompt = \"Write a haiku about a beagle.\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(params, messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cijXEzzVR8RT",
        "outputId": "c79bbaac-abde-41a2-ded0-d1751c65cd63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Ears flop in the wind\nCurious nose leads the way\nBeagle on the hunt"
          },
          "metadata": {}
        }
      ],
      "source": [
        "IPython.display.display(IPython.display.Markdown(response.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Temperature"
      ],
      "metadata": {
        "id": "nwmhE4TvI__c"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36jqmKk7R8RT"
      },
      "source": [
        "\n",
        "| Parameter          | Description                                                                                                                                                   |\n",
        "|--------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| `temperature`      | Controls randomness in the response generation. A higher temperature results in more random responses, while a lower temperature produces more deterministic responses. |\n",
        "\n",
        "Try to modify the temperature from 0 to 1.\n",
        "\n",
        "> In terms of application, you might want to use a lower temperature value for tasks like fact-based QA to encourage more factual and concise responses. For poem generation or other creative tasks, it might be beneficial to increase the temperature value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2OxiV-uQR8RT",
        "outputId": "d218bb8d-32ff-40c6-ca22-dba4e0d7b379",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 890
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response with temperature=0:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Curious beagle nose\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sniffing out adventures near\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Tail wags in delight"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "\n",
            "Response with temperature=0.2:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Curious beagle nose\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sniffing out every scent trail\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Tail wagging with joy"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "\n",
            "Response with temperature=0.4:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Curious beagle nose\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sniffing out adventures near\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Tail wagging with joy"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "\n",
            "Response with temperature=0.6:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Ears flop in the wind,\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Curious nose to the ground,\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Beagle's loyal heart."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "\n",
            "Response with temperature=0.8:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Ears long and drooping\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sniffing out scents in the air\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Beagle's nose never fails"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "\n",
            "Response with temperature=1:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Playful floppy ears,\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sniffing out scents on the breeze,\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Beagle's joy appears."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def split_on_capital(text):\n",
        "    return re.findall(r'[A-Z][^A-Z]*', text)\n",
        "\n",
        "# Experiment with different temperature values\n",
        "for temp in [0, 0.2, 0.4, 0.6, 0.8, 1]:\n",
        "    params = set_open_params(temperature=temp)\n",
        "    response = get_completion(params, messages)\n",
        "    lines = split_on_capital(response.choices[0].message.content.strip())\n",
        "    print(f\"Response with temperature={temp}:\\n\")\n",
        "    for line in lines:\n",
        "        IPython.display.display(IPython.display.Markdown(line))\n",
        "    print(\"-------------------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[View how I used ChatGPT to iterate on this code](https://chat.openai.com/share/6211107a-d868-491b-8a74-ea4debb7a760)"
      ],
      "metadata": {
        "id": "46-IqrqgNFdy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### ðŸ—’ Info: Is it possible to make LLM's deterministic and reproducibile?\n",
        "\n",
        "> It's possible by setting a seed and setting temperature equal to 0. But as mentioned in [OpenAI's docs](https://cookbook.openai.com/examples/reproducible_outputs_with_the_seed_parameter), \"it's important to note that while the seed ensures consistency, it does not guarantee the quality of the output.\""
      ],
      "metadata": {
        "id": "TkSDDqo9iQl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top P\n",
        "\n",
        "Used in nucleus sampling, it defines the probability mass to consider for token generation. A smaller `top_p` leads to more focused sampling.\n",
        "\n",
        "* `top_p` computes the cumulative probability distribution, and cut off as soon as that distribution exceeds the value of `top_p`. **For example, a `top_p` of 0.3 means that only the tokens comprising the top 30% probability mass are considered.**\n",
        "\n",
        "* `top_p` shrinks or grows the \"pool\" of available tokens to choose from, the domain to select over. 1=big pool, 0=small pool. Within that pool, each token has a probability of coming next.\n",
        "\n",
        "Added in a `jinja2` template too (this is optional, but commonly used)."
      ],
      "metadata": {
        "id": "PMEu7ah-R_pP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install textstat"
      ],
      "metadata": {
        "id": "dG82BktkYQfu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "from textstat import flesch_kincaid_grade\n",
        "from jinja2 import Template\n",
        "\n",
        "# Define the prompt using a Jinja template\n",
        "template = Template(\"\"\"\n",
        "{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"{{ prompt }}\"\n",
        "}\n",
        "\"\"\")\n",
        "prompt = \"Generate a unique and creative story idea involving time travel.\"\n",
        "\n",
        "# Function to count unique words\n",
        "def count_unique_words(text):\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    return len(set(words))\n",
        "\n",
        "# Generate the message using the template\n",
        "messages_str = template.render(prompt=prompt)\n",
        "messages = [json.loads(messages_str)]\n",
        "\n",
        "for top_p in [0.1, 0.5, 0.95]:\n",
        "    params = set_open_params(top_p=top_p)\n",
        "    response = get_completion(params, messages)\n",
        "    text = response.choices[0].message.content\n",
        "    unique_word_count = count_unique_words(text)\n",
        "    fk_score = flesch_kincaid_grade(text)\n",
        "    print(f\"Response with top_p={top_p}:\\n\")\n",
        "    print(f\"Unique word count: {unique_word_count}\")\n",
        "    print(f\"Flesch-Kincaid Grade Level: {fk_score}\")\n",
        "    IPython.display.display(IPython.display.Markdown(text))\n",
        "    print(\"-------------------\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        },
        "id": "x-S0B2IHR0Th",
        "outputId": "e2ad7085-7c97-4af9-c43c-5390ade346de"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response with top_p=0.1:\n",
            "\n",
            "Unique word count: 131\n",
            "Flesch-Kincaid Grade Level: 9.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "In the year 3025, time travel has become a common form of entertainment for the wealthy elite. The Time Travel Corporation offers exclusive trips to different eras in history, allowing clients to witness major events firsthand.\n\nOne day, a young woman named Eliza wins a contest to travel back to the year 1920, during the height of the Roaring Twenties. Excited for the opportunity to experience the glitz and glamour of the era, Eliza boards the time machine and is transported back in time.\n\nHowever, upon arrival, Eliza quickly realizes that something is not right. The world she finds herself in is not the glamorous, carefree society she had imagined. Instead, she discovers a dark and dangerous underworld ruled by a powerful crime syndicate.\n\nAs Eliza navigates this treacherous new world, she meets a mysterious man named Jack who claims to be a time traveler from the future. Together, they uncover a sinister plot to alter the course of history and change the future as they know it.\n\nWith the help of Jack and a ragtag group of rebels, Eliza must race against time to stop the crime syndicate and restore the timeline before it's too late. Along the way, she learns valuable lessons about the consequences of meddling with the"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "\n",
            "Response with top_p=0.5:\n",
            "\n",
            "Unique word count: 122\n",
            "Flesch-Kincaid Grade Level: 10.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "In the year 2050, a brilliant scientist named Dr. Amelia Jones invents a revolutionary time machine that allows people to travel back in time to any point in history. However, there is a catch - once someone travels back in time, they cannot return to their original timeline. \n\nDr. Jones decides to test the time machine herself and travels back to the year 1920, where she becomes entangled in a dangerous conspiracy involving a group of time travelers who are trying to alter the course of history for their own gain. As Dr. Jones races against time to stop them, she discovers that the consequences of changing the past are far more devastating than she could have ever imagined.\n\nAs Dr. Jones navigates through different time periods, she begins to uncover the true purpose of the time machine and the dark secrets that lie within it. With the help of a ragtag group of allies from different eras, Dr. Jones must confront her own past mistakes and make the ultimate sacrifice to save the future from being irrevocably altered.\n\nThrough twists and turns, betrayals and alliances, Dr. Jones must navigate the complexities of time travel and ultimately learn that the true power of the time machine lies not in changing the past, but in shaping the future for the better"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "\n",
            "Response with top_p=0.95:\n",
            "\n",
            "Unique word count: 127\n",
            "Flesch-Kincaid Grade Level: 10.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "In the year 3021, time travel has become a popular form of entertainment for the wealthy elite. The Time Travel Corporation offers exclusive trips to different eras in history, giving clients the opportunity to witness monumental events firsthand.\n\nOne day, a young woman named Aurora wins a contest to travel back to ancient Egypt during the construction of the Great Pyramid of Giza. Excited for the adventure, Aurora boards the time machine and is transported back in time.\n\nHowever, when she arrives in ancient Egypt, Aurora quickly realizes that something is wrong. The pyramid is not being built as it should be, and the workers are in a state of panic. As she investigates further, Aurora discovers that a group of rogue time travelers has been altering the past, causing chaos and changing the course of history.\n\nDetermined to set things right, Aurora teams up with a group of rebels from ancient Egypt and embarks on a dangerous journey through time to stop the rogue time travelers and restore the timeline. Along the way, she learns the true power of time travel and the importance of preserving history.\n\nAs Aurora races against the clock to save the past, she also begins to question her own future and the impact her actions will have on the world. Will she be able to stop the rogue time travelers and"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **If you are looking for exact and factual answers keep this low. If you are looking for more diverse responses, increase to a higher value.** If you use Top P it means that only the tokens comprising the top_p probability mass are considered for responses, so a low top_p value selects the most confident responses. This means that a high top_p value will enable the model to look at more possible words, including less likely ones, leading to more diverse outputs. **The general recommendation is to alter temperature or Top P but not both.**"
      ],
      "metadata": {
        "id": "NBhg8_glafVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What day is it?\"\n",
        "\n",
        "# Generate the message using the template and convert it to a dictionary\n",
        "messages_str = template.render(prompt=prompt)\n",
        "messages = [json.loads(messages_str)]\n",
        "\n",
        "for top_p in [0.5, 0.8, 0.95]:\n",
        "    params = set_open_params(top_p=top_p)\n",
        "    response = get_completion(params, messages)\n",
        "    print(f\"Response with top_p={top_p}:\\n\")\n",
        "    IPython.display.display(IPython.display.Markdown(response.choices[0].message.content))\n",
        "    print(\"-------------------\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "zZ-en7l3TRLN",
        "outputId": "e75a0370-5612-4edb-83aa-bf761aeb58af"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response with top_p=0.5:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Today is Wednesday."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "\n",
            "Response with top_p=0.8:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I am an AI digital assistant and do not have the ability to know the current date. Please check your calendar or device for the most accurate information."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "\n",
            "Response with top_p=0.95:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Today is Wednesday."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Max Tokens\n",
        "\n",
        "Defines the maximum length of the generated response measured in tokens (words or pieces of words). It helps in controlling the verbosity of the response.\n",
        "\n",
        "* **Prompt**: \"Explain how photosynthesis works in plants.\"\n",
        "* **Task**: Run the API three times with `max_tokens` set to 50, 100, and 200, respectively."
      ],
      "metadata": {
        "id": "0p8tzYliJCtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment with different max_tokens values\n",
        "prompt = \"Explain how photosynthesis works in plants.\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "for max_tokens in [50, 100, 200]:\n",
        "    params = set_open_params(max_tokens=max_tokens)\n",
        "    response = get_completion(params, messages)\n",
        "    print(f\"Response with max_tokens={max_tokens}:\\n\")\n",
        "    IPython.display.display(IPython.display.Markdown(response.choices[0].message.content))\n",
        "    print(\"-------------------\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "id": "16W-v3mrI-eT",
        "outputId": "73d17702-29dc-428f-8f75-2db795bc20e0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response with max_tokens=50:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Photosynthesis is the process by which plants, algae, and some bacteria use light energy to convert carbon dioxide and water into glucose (sugar) and oxygen. This process occurs in the chloroplasts of plant cells.\n\nFirst, light energy is absorbed"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "\n",
            "Response with max_tokens=100:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Photosynthesis is the process by which plants, algae, and some bacteria convert light energy, usually from the sun, into chemical energy in the form of glucose. This process occurs in the chloroplasts of plant cells.\n\nDuring photosynthesis, plants take in carbon dioxide from the air and water from the soil. Light energy is absorbed by the green pigment chlorophyll, which is located in the chloroplasts. The light energy is used to power a series of chemical reactions that convert the carbon dioxide"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "\n",
            "Response with max_tokens=200:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Photosynthesis is the process by which plants, algae, and some bacteria convert light energy, usually from the sun, into chemical energy in the form of glucose. This process takes place in the chloroplasts of plant cells and involves a series of complex chemical reactions.\n\nDuring photosynthesis, plants take in carbon dioxide from the air and water from the soil. These raw materials are combined with sunlight, which is absorbed by chlorophyll, a green pigment found in chloroplasts. The energy from the sunlight is used to convert the carbon dioxide and water into glucose and oxygen.\n\nThe overall chemical equation for photosynthesis is:\n6CO2 + 6H2O + light energy â†’ C6H12O6 + 6O2\n\nIn this equation, carbon dioxide (CO2) and water (H2O) are combined with light energy to produce glucose (C6H12O6) and oxygen (O2). The glucose produced during photosynthesis is used by the plant as"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiT0Cw4ZR8RT"
      },
      "source": [
        "### 1.1 Text Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "XP7CSLfTR8RT",
        "outputId": "1dd86b0f-3dca-4a8f-e1e5-8edbb249459b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Antibiotics are medicine that helps your body fight off bad germs, but they only work against bacteria, not viruses, so you have to take them the right way to make sure they keep working."
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "params = set_open_params(temperature=0.7)\n",
        "\n",
        "#Original prompt\n",
        "# prompt = \"\"\"Explain the below in one sentence: Antibiotics are a type of medication \\\n",
        "#           used to treat bacterial infections. They work by either killing the bacteria \\\n",
        "#           or preventing them from reproducing, allowing the body's immune system to \\\n",
        "#           fight off the infection. Antibiotics are usually taken orally in the form \\\n",
        "#           of pills, capsules, or liquid solutions, or sometimes administered intravenously. \\\n",
        "#           They are not effective against viral infections, and using them inappropriately \\\n",
        "#           can lead to antibiotic resistance.\"\"\"\n",
        "\n",
        "\n",
        "#This modified prompt worked pretty well\n",
        "# prompt = \"\"\"Explain the below in one sentence: Antibiotics are a type of medication \\\n",
        "#           used to treat bacterial infections. They work by either killing the bacteria \\\n",
        "#           or preventing them from reproducing, allowing the body's immune system to \\\n",
        "#           fight off the infection. Antibiotics are usually taken orally in the form \\\n",
        "#           of pills, capsules, or liquid solutions, or sometimes administered intravenously. \\\n",
        "#           They are not effective against viral infections, and using them inappropriately \\\n",
        "#           can lead to antibiotic resistance. ELI5 literally\"\"\"\n",
        "\n",
        "#Modified prompt\n",
        "prompt = \"\"\"Explain the below in one sentence: Antibiotics are a type of medication \\\n",
        "          used to treat bacterial infections. They work by either killing the bacteria \\\n",
        "          or preventing them from reproducing, allowing the body's immune system to \\\n",
        "          fight off the infection. Antibiotics are usually taken orally in the form \\\n",
        "          of pills, capsules, or liquid solutions, or sometimes administered intravenously. \\\n",
        "          They are not effective against viral infections, and using them inappropriately \\\n",
        "          can lead to antibiotic resistance. Explain like I am 5\"\"\"\n",
        "\n",
        "# Generate the message using the template and convert it to a dictionary\n",
        "messages_str = template.render(prompt=prompt)\n",
        "messages = [json.loads(messages_str)]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzOToTAxR8RT"
      },
      "source": [
        "> **Exercise**: Instruct the model to explain the paragraph in one sentence like \"I am 5\". Do you see any differences?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pExMtUo-R8RT"
      },
      "source": [
        "### 1.2 Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "X8lYXWErR8RT",
        "outputId": "e06c69df-3d94-4694-ab4a-4f279828433e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Unsure about answer."
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# prompt = \"\"\"Answer the question based on the context below. \\\n",
        "#  Keep the answer short and concise. \\\n",
        "#  Respond 'Unsure about answer' if not sure about the answer. \\\n",
        "#  Context: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. \\\n",
        "#  There, scientists generated an early version of the antibody, dubbed OKT3. \\\n",
        "#  Originally sourced from mice, the molecule was able to bind to the surface of T cells \\\n",
        "#  and limit their cell-killing potential. In 1986, it was approved to help prevent organ \\\n",
        "#  rejection after kidney transplants, making it the first therapeutic antibody allowed \\\n",
        "#  for human use. \\\n",
        "#  Question: What was OKT3 originally sourced from? \\\n",
        "#  Answer:\"\"\"\n",
        "\n",
        "\n",
        "# This prompt often still answers \"mice\" or \"mice (Unsure about answer)\" but\n",
        "# also seems to produce the desired result around 30% of the time\n",
        "# prompt = \"\"\"Answer the question based on the context below. \\\n",
        "#  Keep the answer short and concise. \\\n",
        "#  Respond 'Unsure about answer' if the answer isn't absolutely, completely certain. \\\n",
        "#  Context: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. \\\n",
        "#  There, scientists generated an early version of the antibody, dubbed OKT3. \\\n",
        "#  Supposedly originally sourced from mice, the molecule was able to bind to the surface of T cells \\\n",
        "#  and limit their cell-killing potential. However the original source can't be verified. \\\n",
        "#  In 1986, it was approved to help prevent organ \\\n",
        "#  rejection after kidney transplants, making it the first therapeutic antibody allowed \\\n",
        "#  for human use. \\\n",
        "#  Question: What was OKT3 originally sourced from? \\\n",
        "#  Answer:\"\"\"\n",
        "\n",
        "\n",
        "# Adding the phrase \"OK3 is not currently obtained from mice.\" led the model to\n",
        "# admit it's unsure about the origin of OKT3 fairly consistently, despite having\n",
        "# no direct bearing on that question\n",
        "prompt = \"\"\"Answer the question based on the context below. \\\n",
        " Keep the answer short and concise. \\\n",
        " Respond 'Unsure about answer' if the answer isn't absolutely, completely certain. \\\n",
        " Context: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. \\\n",
        " There, scientists generated an early version of the antibody, dubbed OKT3. \\\n",
        " Supposedly originally sourced from mice, the molecule was able to bind to the surface of T cells \\\n",
        " and limit their cell-killing potential. However the original source can't be verified. \\\n",
        " In 1986, it was approved to help prevent organ \\\n",
        " rejection after kidney transplants, making it the first therapeutic antibody allowed \\\n",
        " for human use. OKT3 is not currently obtained from mice. \\\n",
        " Question: What was OKT3 originally sourced from? \\\n",
        " Answer:\"\"\"\n",
        "\n",
        "# Generate the message using the template and convert it to a dictionary\n",
        "messages_str = template.render(prompt=prompt)\n",
        "messages = [json.loads(messages_str)]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLg9Z2i4R8RT"
      },
      "source": [
        "Context obtained from here: https://www.nature.com/articles/d41586-023-00400-x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrtEMAPsR8RU"
      },
      "source": [
        "> **Exercise**: Edit prompt and get the model to respond that it isn't sure about the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dso3RN7sR8RU"
      },
      "source": [
        "### 1.3 Text Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wjw0n9rFR8RU",
        "outputId": "c39cf5f5-a0ee-45b6-f326-fdf67862a462",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Positive. \nThe text expresses a positive sentiment by stating that the food was amazing. This indicates that the person had a great experience with the food and enjoyed it."
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Original prompt\n",
        "# prompt = \"\"\"Classify the text into neutral, negative or positive. \\\n",
        "#                                                   \\\n",
        "#             Text: I think the food was amazing!   \\\n",
        "#                                                   \\\n",
        "#             Sentiment:\"\"\"\n",
        "\n",
        "prompt = \"\"\"Classify the text into neutral, negative or positive. \\\n",
        "            Then explain the reasons for the classification.      \\\n",
        "                                                  \\\n",
        "            Text: I think the food was amazing!   \\\n",
        "                                                  \\\n",
        "            Sentiment:                            \\\n",
        "            Explanation:\"\"\"\n",
        "\n",
        "# Generate the message using the template and convert it to a dictionary\n",
        "messages_str = template.render(prompt=prompt)\n",
        "messages = [json.loads(messages_str)]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ti6BNfnR8RU"
      },
      "source": [
        "> **Exercise**: Modify the prompt to instruct the model to provide an explanation to the answer selected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apKYpWJMR8RU"
      },
      "source": [
        "### 1.4 Role Playing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5c-L0twDR8RU",
        "outputId": "b0cd826a-5a95-43cd-b3c3-6c649cff1986",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Black holes form from collapsing stars."
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "#Original prompt\n",
        "# prompt = \"\"\"The following is a conversation with an AI research assistant. \\\n",
        "# The assistant tone is technical and scientific. \\\n",
        "#                                                 \\\n",
        "# Human: Hello, who are you? \\\n",
        "# AI: Greeting! I am an AI research assistant. How can I help you today? \\\n",
        "# Human: Can you tell me about the creation of blackholes? \\\n",
        "# AI:\"\"\"\n",
        "\n",
        "# Direction to be concise - interestingly, this was achieved despite not\n",
        "# making the response length limit explicit\n",
        "prompt = \"\"\"The following is a conversation with an AI research assistant. \\\n",
        "The assistant tone is technical and scientific. The assistant limits the length\\\n",
        "of its responses to the absolute minimum number of words necessary to answer   \\\n",
        "the question.                                                                  \\\n",
        "Human: Hello, who are you? \\\n",
        "AI: Greeting! I am an AI research assistant. How can I help you today? \\\n",
        "Human: Can you tell me about the creation of blackholes? \\\n",
        "AI:\"\"\"\n",
        "\n",
        "# Generate the message using the template and convert it to a dictionary\n",
        "messages_str = template.render(prompt=prompt)\n",
        "messages = [json.loads(messages_str)]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GTLlgAaR8RU"
      },
      "source": [
        "> **Exercise**: Modify the prompt to instruct the model to keep AI responses concise and short."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH2Xt1dTR8RU"
      },
      "source": [
        "### 1.5 Code Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "cQpcLZ3cR8RU",
        "outputId": "9722780d-34fb-4a0b-ab34-6298297612a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "import random\n\nclass VirtualPlantCareAssistant:\n    def __init__(self, plant_name):\n        self.plant_name = plant_name\n        self.water_level = 50\n        self.sunlight_level = 50\n        self.health = 100\n\n    def check_status(self):\n        print(f\"{self.plant_name} status:\")\n        print(f\"Water level: {self.water_level}\")\n        print(f\"Sunlight level: {self.sunlight_level}\")\n        print(f\"Health: {self.health}\")\n\n    def water_plant(self):\n        self.water_level += random.randint(5, 15)\n        print(f\"{self.plant_name} has been watered. Water level is now {self.water_level}.\")\n\n    def give_sunlight(self):\n        self.sunlight_level += random.randint(5, 15)\n        print(f\"{self.plant_name} has been given sunlight. Sunlight level is now {self.sunlight_level}.\")\n\n    def check_health(self):\n        if self.water_level < 30 or self.sunlight_level < 30:\n            self.health -= random.randint(5, 10)\n            print(f\"{self.plant_name} is not getting enough water or sunlight. Health has decreased to {self.health}."
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "#Original prompt\n",
        "# prompt = \"\"\"Develop a small Python API that functions as a virtual plant care assistant.\"\"\"\n",
        "\n",
        "prompt = \"\"\"Develop a small Python API that functions as a virtual plant care assistant.\"\"\"\n",
        "\n",
        "\n",
        "# Generate the message using the template and convert it to a dictionary\n",
        "messages_str = template.render(prompt=prompt)\n",
        "messages = [json.loads(messages_str)]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfjREa5MR8RU"
      },
      "source": [
        "### 1.6 Reasoning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "dZAwJhQ9R8RU",
        "outputId": "81c73add-1038-4474-c8f1-d26b57e7f297",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- Odd numbers in the group: 15, 5, 13, 7, 1\n- Add the odd numbers together: 15 + 5 + 13 + 7 + 1 = 41\n- The result (41) is an odd number."
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# Original prompt\n",
        "# prompt = \"\"\"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \\\n",
        "#                                                                                               \\\n",
        "# Solve by breaking the problem into steps.                                                     \\\n",
        "#                                                                                               \\\n",
        "# First, identify the odd numbers, add them, and indicate whether the result is odd or even.\"\"\"\n",
        "\n",
        "#Modified prompt\n",
        "prompt = \"\"\"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \\\n",
        "Solve by breaking the problem into steps. At each step, explain what you are doing and why. \\\n",
        "First, identify the odd numbers. Second, add the odd numbers together. \\\n",
        "Finally, indicate whether the result is odd or even. Put each step on a new line. \"\"\"\n",
        "\n",
        "\n",
        "# Generate the message using the template and convert it to a dictionary\n",
        "messages_str = template.render(prompt=prompt)\n",
        "messages = [json.loads(messages_str)]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYVo5K_gR8RU"
      },
      "source": [
        "> **Exercise**: Improve the prompt to have a better structure and output format."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.7 Brain storming"
      ],
      "metadata": {
        "id": "nabOY4InE3Rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Generate 10 different names for a grunge rock bands based on dog names. Keep it to only 2 word band names.\"\"\"\n",
        "\n",
        "# Generate the message using the template and convert it to a dictionary\n",
        "messages_str = template.render(prompt=prompt)\n",
        "messages = [json.loads(messages_str)]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "KWNpTyvIE1q5",
        "outputId": "3691efe2-af44-422b-e0d5-bde7d9f4752b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "1. Bark Riot\n2. Howl Brigade\n3. Ruff Rebellion\n4. Snarl Syndicate\n5. Growl Generation\n6. Woof Warriors\n7. Paws Protest\n8. Fierce Fidos\n9. Rebel Hounds\n10. Grunge Pups"
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.8 JSON generation\n",
        "\n",
        "Another new feature in OpenAI is [JSON mode](https://platform.openai.com/docs/guides/text-generation/json-mode).\n",
        "\n"
      ],
      "metadata": {
        "id": "GOLmKaFaGoO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  response_format={ \"type\": \"json_object\" },\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Generate 5 doctor's notes as a 'text' field that precribes drugs, dosages, form, duration, and frequency. Then in JSON have an 'entities' nested dictionary with each of the entities\"}\n",
        "  ]\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQe3ytShIdaH",
        "outputId": "e5925587-4fbf-4d41-e22a-dfb5b1338039"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"doctor_notes\": [\n",
            "        {\n",
            "            \"text\": \"Prescription: Ibuprofen 200mg tablets. Dosage: Take 1 tablet every 4-6 hours as needed for pain. Form: Tablets. Duration: 5 days. Frequency: As needed.\",\n",
            "            \"entities\": {\n",
            "                \"prescription\": \"Ibuprofen 200mg tablets\",\n",
            "                \"dosage\": \"1 tablet every 4-6 hours\",\n",
            "                \"form\": \"Tablets\",\n",
            "                \"duration\": \"5 days\",\n",
            "                \"frequency\": \"As needed\"\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"text\": \"Prescription: Amoxicillin 500mg capsules. Dosage: Take 1 capsule three times a day. Form: Capsules. Duration: 7 days. Frequency: With meals.\",\n",
            "            \"entities\": {\n",
            "                \"prescription\": \"Amoxicillin 500mg capsules\",\n",
            "                \"dosage\": \"1 capsule three times a day\",\n",
            "                \"form\": \"Capsules\",\n",
            "                \"duration\": \"7 days\",\n",
            "                \"frequency\": \"With meals\"\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"text\": \"Prescription: Simvastatin 20mg tablets. Dosage: Take 1 tablet daily in the evening. Form: Tablets. Duration: Indefinitely. Frequency: Daily.\",\n",
            "            \"entities\": {\n",
            "                \"prescription\": \"Simvastatin 20mg tablets\",\n",
            "                \"dosage\": \"1 tablet daily in the evening\",\n",
            "                \"form\": \"Tablets\",\n",
            "                \"duration\": \"Indefinitely\",\n",
            "                \"frequency\": \"Daily\"\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"text\": \"Prescription: Albuterol 2mg inhaler. Dosage: Use 2 puffs every 4-6 hours as needed for shortness of breath. Form: Inhaler. Duration: As needed. Frequency: Every 4-6 hours.\",\n",
            "            \"entities\": {\n",
            "                \"prescription\": \"Albuterol 2mg inhaler\",\n",
            "                \"dosage\": \"2 puffs every 4-6 hours\",\n",
            "                \"form\": \"Inhaler\",\n",
            "                \"duration\": \"As needed\",\n",
            "                \"frequency\": \"Every 4-6 hours\"\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"text\": \"Prescription: Prednisone 10mg tablets. Dosage: Take 2 tablets in the morning with food. Form: Tablets. Duration: 10 days. Frequency: Once daily.\",\n",
            "            \"entities\": {\n",
            "                \"prescription\": \"Prednisone 10mg tablets\",\n",
            "                \"dosage\": \"2 tablets in the morning with food\",\n",
            "                \"form\": \"Tablets\",\n",
            "                \"duration\": \"10 days\",\n",
            "                \"frequency\": \"Once daily\"\n",
            "            }\n",
            "        }\n",
            "    ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### ðŸ—’ Info\n",
        "> - When using JSON mode, always instruct the model to produce JSON via some message in the conversation, for example via your system message. If you don't include an explicit instruction to generate JSON, the model may generate an unending stream of whitespace and the request may run continually until it reaches the token limit. To help ensure you don't forget, the API will throw an error if the string \"JSON\" does not appear somewhere in the context.\n",
        "\n",
        "> - The JSON in the message the model returns may be partial (i.e. cut off) if finish_reason is length, which indicates the generation exceeded max_tokens or the conversation exceeded the token limit. To guard against this, check finish_reason before parsing the response.\n",
        "\n",
        "> - JSON mode will not guarantee the output matches any specific schema, only that it is valid and parses without errors."
      ],
      "metadata": {
        "id": "ytuw_jmKMxnu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoC_k0P2R8RU"
      },
      "source": [
        "## 2. Advanced Prompting Techniques\n",
        "\n",
        "Objectives:\n",
        "\n",
        "- Cover more advanced techniques for prompting: few-shot, chain-of-thoughts,..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtLFfYkMR8RU"
      },
      "source": [
        "### 2.2 Few-shot prompts\n",
        "\n",
        "[Default v2 Prompt from Prodigy](https://prodi.gy/docs/large-language-models#more-config)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "You are an expert Named Entity Recognition (NER) system. Your task is to accept Text as input and extract named entities for the set of predefined entity labels.\n",
        "\n",
        "From the Text input provided, extract named entities for each label in the following format:\n",
        "\n",
        "DISH: <comma delimited list of strings>\n",
        "INGREDIENT: <comma delimited list of strings>\n",
        "EQUIPMENT: <comma delimited list of strings>\n",
        "\n",
        "Below are definitions of each label to help aid you in what kinds of named entities to extract for each label.\n",
        "Assume these definitions are written by an expert and follow them closely.\n",
        "\n",
        "DISH: Extract the name of a known dish.\n",
        "INGREDIENT: Extract the name of a cooking ingredient, including herbs and spices.\n",
        "EQUIPMENT: Extract any mention of cooking equipment. e.g. oven, cooking pot, grill\n",
        "\n",
        "Below are some examples (only use these as a guide):\n",
        "\n",
        "Text:\n",
        "'''\n",
        "You can't get a great chocolate flavor with carob.\n",
        "'''\n",
        "\n",
        "INGREDIENT: carob\n",
        "\n",
        "Text:\n",
        "'''\n",
        "You can probably sand-blast it if it's an anodized aluminum pan.\n",
        "'''\n",
        "\n",
        "INGREDIENT:\n",
        "EQUIPMENT: anodized aluminum pan\n",
        "\n",
        "\n",
        "Here is the text that needs labeling:\n",
        "\n",
        "Text:\n",
        "'''\n",
        "In Silicon Valley, a Voice of Caution Guides a High-Flying Uber\n",
        "'''\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "OttnGJApfbKS",
        "outputId": "42456a48-1fc3-4a58-c161-f29aada9a227"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "DISH: \nINGREDIENT: Silicon Valley\nEQUIPMENT: Uber"
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "t83KToQBR8RU",
        "outputId": "0be24e56-faab-4bbd-d1aa-125f687ffc81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The sum of the odd numbers in this group is 15 + 5 + 13 + 7 + 1 = 41, which is an odd number."
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "prompt = \"\"\"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "A:\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "W4PTuArsR8RU",
        "outputId": "814b9b83-8a6b-4d52-e5ce-7033363a3d07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The answer is False."
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "prompt = \"\"\"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
        "A: The answer is False.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
        "A: The answer is True.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
        "A: The answer is True.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
        "A: The answer is False.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "A:\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAzMuH49R8RU"
      },
      "source": [
        "### 2.3 Chain-of-Thought (CoT) Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "g3G4aRcdR8RU",
        "outputId": "0306413f-4f1b-4812-dc4d-978b3ba2bfcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "1. Initially, you bought 10 apples.\n2. You gave 2 apples to the neighbor and 2 to the repairman, leaving you with 10 - 2 - 2 = 6 apples.\n3. You then bought 5 more apples, so you had 6 + 5 = 11 apples.\n4. Finally, you ate 1 apple, so you remained with 11 - 1 = 10 apples.\n\nTherefore, you remained with 10 apples."
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "prompt = \"\"\"I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
        "\n",
        "Let's think step by step.\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4FVm7NsR8RY"
      },
      "source": [
        "### 2.5 Self-Consistency\n",
        "As an **optional** exercise, check examples in our [guide](https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/guides/prompts-advanced-usage.md#self-consistency) and try them here.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat actually responds correctly, pretty consistently\n",
        "# Sometimes, it provides an explanation\n",
        "prompt = \"\"\"When I was 6 my sister was half my age. Now\\\n",
        "          Iâ€™m 70 how old is my sister?\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "KsBU8OXeUNPF",
        "outputId": "ebff6ab4-2239-4c64-8e4d-41d1e3cfddfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "If you were 6 and your sister was half your age, that means she was 3 years old. Since then, 64 years have passed (70 - 6), so your sister would now be 67 years old (3 + 64)."
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Similar performance\n",
        "prompt = \"\"\"Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,\n",
        "there will be 21 trees. How many trees did the grove workers plant today?\n",
        "A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted.\n",
        "So, they must have planted 21 - 15 = 6 trees. The answer is 6.\n",
        "\n",
        "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
        "A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n",
        "\n",
        "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
        "A: Leah had 32 chocolates and Leahâ€™s sister had 42. That means there were originally 32 + 42 = 74\n",
        "chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\n",
        "\n",
        "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops\n",
        "did Jason give to Denny?\n",
        "A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of\n",
        "lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\n",
        "\n",
        "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does\n",
        "he have now?\n",
        "A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so\n",
        "in total he has 7 + 2 = 9 toys. The answer is 9.\n",
        "\n",
        "Q: There were nine computers in the server room. Five more computers were installed each day, from\n",
        "monday to thursday. How many computers are now in the server room?\n",
        "A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 =\n",
        "20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers.\n",
        "The answer is 29.\n",
        "\n",
        "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many\n",
        "golf balls did he have at the end of wednesday?\n",
        "A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On\n",
        "Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\n",
        "\n",
        "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
        "A: She bought 5 bagels for $3 each. This means she spent 5\n",
        "\n",
        "Q: When I was 6 my sister was half my age. Now Iâ€™m 70 how old is my sister?\n",
        "A:\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "iizjJrCPU5iu",
        "outputId": "94c7d9a9-2837-4f92-fc03-dbb0f57f5825",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "When you were 6, your sister was half your age, so she was 3 years old. The age difference between you and your sister is 3 years. Now that you are 70, your sister must be 70 - 3 = 67 years old."
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6 Generate Knowledge Prompting\n",
        "\n",
        "As an **optional** exercise, check examples in our [guide](https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/guides/prompts-advanced-usage.md#generated-knowledge-prompting) and try them here."
      ],
      "metadata": {
        "id": "hLYQeDW6UL25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat now responds correctly to this question, as well\n",
        "prompt = \"\"\"Part of golf is trying to get a higher point total than others. Yes or No?\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "L-f4unXeUN7N",
        "outputId": "0c6ce6b2-9b7c-4771-8a58-7afbbb5f6031",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "No. In golf, the goal is to have the lowest score possible, not the highest."
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat corrects the user's misconception about golf\n",
        "prompt = \"\"\"Input: Greece is larger than mexico.\n",
        "Knowledge: Greece is approximately 131,957 sq km, while Mexico is approximately 1,964,375 sq km, making Mexico 1,389% larger than Greece.\n",
        "\n",
        "Input: Glasses always fog up.\n",
        "Knowledge: Condensation occurs on eyeglass lenses when water vapor from your sweat, breath, and ambient humidity lands on a cold surface, cools, and then changes into tiny drops of liquid, forming a film that you see as fog. Your lenses will be relatively cool compared to your breath, especially when the outside air is cold.\n",
        "\n",
        "Input: A fish is capable of thinking.\n",
        "Knowledge: Fish are more intelligent than they appear. In many areas, such as memory, their cognitive powers match or exceed those of â€™higherâ€™ vertebrates including non-human primates. Fishâ€™s long-term memories help them keep track of complex social relationships.\n",
        "\n",
        "Input: A common effect of smoking lots of cigarettes in oneâ€™s lifetime is a higher than normal chance of getting lung cancer.\n",
        "Knowledge: Those who consistently averaged less than one cigarette per day over their lifetime had nine times the risk of dying from lung cancer than never smokers. Among people who smoked between one and 10 cigarettes per day, the risk of dying from lung cancer was nearly 12 times higher than that of never smokers.\n",
        "\n",
        "Input: A rock is the same size as a pebble.\n",
        "Knowledge: A pebble is a clast of rock with a particle size of 4 to 64 millimetres based on the Udden-Wentworth scale of sedimentology. Pebbles are generally considered larger than granules (2 to 4 millimetres diameter) and smaller than cobbles (64 to 256 millimetres diameter).\n",
        "\n",
        "Input: Part of golf is trying to get a higher point total than others.\n",
        "Knowledge:\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "nlQAux17WAqK",
        "outputId": "8de3ed25-cc76-4b6e-cc38-3e7439003253",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "In golf, the objective is to have the lowest score possible, not the highest. Players aim to complete each hole in the fewest number of strokes, with the overall goal being to have the lowest total score at the end of the round."
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3 LangChain\n",
        "\n",
        "This is adopted from [this notebook](https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/notebooks/pe-chatgpt-langchain.ipynb)."
      ],
      "metadata": {
        "id": "La1xBnKINg3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")"
      ],
      "metadata": {
        "id": "2gsAdU8SNfj7"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chat mode instance\n",
        "chat = ChatOpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "mERx9bWONk82"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Seems to return positive consistently, as expected\n",
        "USER_INPUT = \"I love programming.\"\n",
        "FINAL_PROMPT = \"\"\"Classify the text into neutral, negative or positive.\n",
        "\n",
        "Text: {user_input}.\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "chat.invoke([HumanMessage(content=FINAL_PROMPT.format(user_input=USER_INPUT))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zMTQOuONsQH",
        "outputId": "ce606c44-04ec-4f93-a311-054e9e98d3d1"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Positive', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 27, 'total_tokens': 28}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d04574b6-39ef-44db-9873-b551ddfb131f-0', usage_metadata={'input_tokens': 27, 'output_tokens': 1, 'total_tokens': 28})"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUIKyClUR8RY"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "promptlecture",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "f38e0373277d6f71ee44ee8fea5f1d408ad6999fda15d538a69a99a1665a839d"
      }
    },
    "colab": {
      "provenance": [],
      "name": "pe-openai-lecture.ipynb",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}