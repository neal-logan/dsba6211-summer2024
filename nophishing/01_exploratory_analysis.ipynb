{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neal-logan/dsba6211-summer2024/blob/main/nophishing/01_exploratory_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 01 Exploratory Analysis\n"
      ],
      "metadata": {
        "id": "RET7C16wNHx6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "CUVYV2jdLoy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Required Packages\n",
        "Developed with Python version 3.10.12 in Colab version 1.0.0"
      ],
      "metadata": {
        "id": "c7SzEAhKz6C4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##https://github.com/facebookresearch/hiplot\n",
        "!pip install hiplot==0.1.33"
      ],
      "metadata": {
        "id": "80KczabzVNle",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afe3fe63-8bdf-42b7-82ed-ad743fa1721a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hiplot==0.1.33\n",
            "  Downloading hiplot-0.1.33-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: ipython>=7.0.1 in /usr/local/lib/python3.10/dist-packages (from hiplot==0.1.33) (7.34.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from hiplot==0.1.33) (2.2.5)\n",
            "Collecting flask-compress (from hiplot==0.1.33)\n",
            "  Downloading Flask_Compress-1.15-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from hiplot==0.1.33) (4.12.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.0.1->hiplot==0.1.33) (71.0.4)\n",
            "Collecting jedi>=0.16 (from ipython>=7.0.1->hiplot==0.1.33)\n",
            "  Using cached jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=7.0.1->hiplot==0.1.33) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=7.0.1->hiplot==0.1.33) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.0.1->hiplot==0.1.33) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.0.1->hiplot==0.1.33) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=7.0.1->hiplot==0.1.33) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=7.0.1->hiplot==0.1.33) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=7.0.1->hiplot==0.1.33) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.0.1->hiplot==0.1.33) (4.9.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->hiplot==0.1.33) (2.5)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask->hiplot==0.1.33) (3.0.3)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask->hiplot==0.1.33) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->hiplot==0.1.33) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->hiplot==0.1.33) (8.1.7)\n",
            "Collecting brotli (from flask-compress->hiplot==0.1.33)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting zstandard (from flask-compress->hiplot==0.1.33)\n",
            "  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=7.0.1->hiplot==0.1.33) (0.8.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask->hiplot==0.1.33) (2.1.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=7.0.1->hiplot==0.1.33) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.0.1->hiplot==0.1.33) (0.2.13)\n",
            "Downloading hiplot-0.1.33-py3-none-any.whl (863 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m863.2/863.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Flask_Compress-1.15-py3-none-any.whl (8.6 kB)\n",
            "Using cached jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: brotli, zstandard, jedi, flask-compress, hiplot\n",
            "Successfully installed brotli-1.1.0 flask-compress-1.15 hiplot-0.1.33 jedi-0.19.1 zstandard-0.23.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#https://github.com/SelfExplainML/PiML-Toolbox\n",
        "!pip install PiML==0.6.0"
      ],
      "metadata": {
        "id": "y1eSP4w8aNc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random Seed"
      ],
      "metadata": {
        "id": "JzAAYv0dElgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed = 42"
      ],
      "metadata": {
        "id": "ZKOK1l8gEnfJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load and Prepare Data"
      ],
      "metadata": {
        "id": "hsdvzxeCXAR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and prepare training data\n",
        "import pandas as pd\n",
        "\n",
        "train_url = 'https://raw.githubusercontent.com/neal-logan/dsba6211-summer2024/main/nophishing/data/phishing-url-pirochet-train.csv'\n",
        "df = pd.read_csv(train_url)\n",
        "\n",
        "#Create numeric target variable column\n",
        "df['y'] = df['status'].replace('legitimate', 0).replace('phishing', 1)\n",
        "\n",
        "#Drop unnecessary columns\n",
        "df = df.drop(columns=['status','url'])\n",
        "\n",
        "#X/y split\n",
        "X = df.drop(columns=['y'])\n",
        "y = df['y']\n"
      ],
      "metadata": {
        "id": "jOS-vzfHNouy"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split training set into training and validation set (test set not yet loaded)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_validation, y_train, y_validation = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size = 0.2,\n",
        "    random_state = random_seed)"
      ],
      "metadata": {
        "id": "MOc4Zid9VnmV"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Establish a list of columns being dropped from exploration and modeling\n",
        "column_drop_list = ['url','status']"
      ],
      "metadata": {
        "id": "dgK2sOE5GcJ2"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overview of the Data\n",
        "\n",
        "Most of the columns are a mix of binary, small discrete numbers, or ratios in decimal format.  Some columns have significantly larger values.  Some columns contain negative values that are apparently invalid.\n",
        "\n",
        "While the column names follow some degree of convention, there's no simple way to delineate how each of them should be handled.  I will need to analyze each feature individually and organize a preprocessing pipeline that takes into account what each feature needs.\n",
        "\n",
        "Some of the fields are obtained from third-party providers.  Because these might not always be available, I will develop a set of models that use this data as well as a second set of models that do not."
      ],
      "metadata": {
        "id": "D77WZoKCNQgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Categorize Features by Source"
      ],
      "metadata": {
        "id": "JpiuKe2wUXnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split features into categories based on origin: from URL, from site content,\n",
        "# or from third parties\n",
        "\n",
        "url_columns = ['length_url', 'length_hostname', 'ip', 'nb_dots',\n",
        "  'nb_hyphens', 'nb_at', 'nb_qm', 'nb_and', 'nb_or', 'nb_eq',\n",
        "  'nb_underscore', 'nb_tilde', 'nb_percent', 'nb_slash', 'nb_star',\n",
        "  'nb_colon', 'nb_comma', 'nb_semicolumn', 'nb_dollar', 'nb_space',\n",
        "  'nb_www', 'nb_com', 'nb_dslash', 'http_in_path', 'https_token',\n",
        "  'ratio_digits_url', 'ratio_digits_host', 'punycode', 'port',\n",
        "  'tld_in_path', 'tld_in_subdomain', 'abnormal_subdomain',\n",
        "  'nb_subdomains', 'prefix_suffix', 'random_domain', 'shortening_service',\n",
        "  'path_extension', 'nb_redirection', 'nb_external_redirection',\n",
        "  'length_words_raw', 'char_repeat', 'shortest_words_raw',\n",
        "  'shortest_word_host', 'shortest_word_path', 'longest_words_raw',\n",
        "  'longest_word_host', 'longest_word_path', 'avg_words_raw',\n",
        "  'avg_word_host', 'avg_word_path', 'phish_hints', 'domain_in_brand',\n",
        "  'brand_in_subdomain', 'brand_in_path', 'suspecious_tld',\n",
        "  'statistical_report' ]\n",
        "\n",
        "site_content_columns = ['nb_hyperlinks', 'ratio_intHyperlinks',\n",
        "  'ratio_extHyperlinks', 'ratio_nullHyperlinks', 'nb_extCSS',\n",
        "  'ratio_intRedirection', 'ratio_extRedirection', 'ratio_intErrors',\n",
        "  'ratio_extErrors', 'login_form', 'external_favicon', 'links_in_tags',\n",
        "  'submit_email', 'ratio_intMedia', 'ratio_extMedia', 'sfh', 'iframe',\n",
        "  'popup_window', 'safe_anchor', 'onmouseover', 'right_clic',\n",
        "  'empty_title', 'domain_in_title', 'domain_with_copyright']\n",
        "\n",
        "third_party_columns = ['whois_registered_domain', 'domain_registration_length', 'domain_age',\n",
        "       'web_traffic', 'dns_record', 'google_index', 'page_rank']\n",
        "\n",
        "\n",
        "X_url_train = X_train[url_columns]\n",
        "X_site_content_train = X_train[site_content_columns]\n",
        "X_third_party_train = X_train[third_party_columns]"
      ],
      "metadata": {
        "id": "7SIDsnEuUdti"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feature-Target Correlation\n",
        "\n",
        "Third-party features google_index and page_rank dominate the correlations.  However, url features nb_www and ratio_digits_url also look promising, and they're closely followed by site content variables domain_in_title and nb_hyperlinks.\n",
        "\n",
        "On the other hand, most features show very little correlation, including several under 0.01.  And 5 features show no variation at all after removing the validation set from the training set.\n",
        "\n",
        "While the uncorrelated features could still plausibly be useful to decision tree models, features that do not vary can't be of any use and will be dropped.\n",
        "\n",
        "On closer inspection,\n"
      ],
      "metadata": {
        "id": "grh_DRcvFoeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recombine X,y training data sets for exploration\n",
        "Xy_train = X_train.copy()\n",
        "Xy_train['y'] = y_train"
      ],
      "metadata": {
        "id": "D9US3ozmHoS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate correlation matrix\n",
        "corr_matrix = Xy_train.corr().abs()\n",
        "\n",
        "# Get top features correlated with the target variable\n",
        "# top_correlated_columns = corr_matrix['y'].sort_values(ascending=False).head(10)\n",
        "print(corr_matrix['y'].sort_values(ascending=False).head(45))\n",
        "print(corr_matrix['y'].sort_values(ascending=False).tail(45))\n",
        "\n"
      ],
      "metadata": {
        "id": "pquoX8SGazEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify non-varying features\n",
        "non_varying_columns = []\n",
        "for col in X_train.columns:\n",
        "  if len(pd.unique(X_train[col])) < 2:\n",
        "    non_varying_columns.append(col)\n",
        "\n",
        "print(non_varying_columns)"
      ],
      "metadata": {
        "id": "8YLz3b1aDam8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add non-varying features to the drop list & drop from X_train\n",
        "column_drop_list.extend(non_varying_columns)\n",
        "\n",
        "X_train.drop(columns=non_varying_columns, inplace=True)\n",
        "X_validation.drop(columns=non_varying_columns, inplace=True)\n",
        "Xy_train.drop(columns=non_varying_columns, inplace=True)\n",
        "\n",
        "# print(column_drop_list)\n"
      ],
      "metadata": {
        "id": "yyFvjZQpDk35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Source-specific lists"
      ],
      "metadata": {
        "id": "mi498hOcGvTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create source-specific lists\n",
        "\n",
        "# Xy_url_train = X_url_train.copy()\n",
        "# Xy_url_train['y'] = y_train\n",
        "\n",
        "# Xy_site_content_train = X_site_content_train.copy()\n",
        "# Xy_site_content_train['y'] = y_train\n",
        "\n",
        "# Xy_third_party_train = X_third_party_train.copy()\n",
        "# Xy_third_party_train['y'] = y_train"
      ],
      "metadata": {
        "id": "BCeHuPfJUjZo"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate correlation matrix\n",
        "# corr_matrix_url = Xy_url_train.corr().abs()\n",
        "\n",
        "# # Get top 10 url features correlated with the target variable\n",
        "# top_correlated_url_columns = corr_matrix_url['y'].sort_values(ascending=False).head(10)\n",
        "# print(top_correlated_url_columns)"
      ],
      "metadata": {
        "id": "rPEyXDR3HW0Z",
        "outputId": "ff174bcd-d244-4e98-a288-654c1de2edb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y                    1.000000\n",
            "nb_www               0.434337\n",
            "ratio_digits_url     0.357516\n",
            "phish_hints          0.326646\n",
            "ip                   0.316615\n",
            "nb_qm                0.301241\n",
            "nb_slash             0.246719\n",
            "length_hostname      0.243607\n",
            "length_url           0.231634\n",
            "ratio_digits_host    0.227973\n",
            "Name: y, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# corr_matrix_site_content = Xy_site_content_train.corr().abs()\n",
        "\n",
        "# # Get top 10 url features correlated with the target variable\n",
        "# top_correlated_site_content_columns = corr_matrix_site_content['y'].sort_values(ascending=False).head(10)\n",
        "# print(top_correlated_site_content_columns)"
      ],
      "metadata": {
        "id": "HymERhFeHWs-",
        "outputId": "85cdbfcc-9279-403f-c50d-fdf33ff8c0a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y                        1.000000\n",
            "domain_in_title          0.338552\n",
            "nb_hyperlinks            0.334818\n",
            "ratio_intHyperlinks      0.255428\n",
            "empty_title              0.221384\n",
            "ratio_intMedia           0.202347\n",
            "links_in_tags            0.191689\n",
            "safe_anchor              0.190779\n",
            "domain_with_copyright    0.180982\n",
            "ratio_extRedirection     0.163749\n",
            "Name: y, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# corr_matrix_third_party = Xy_third_party_train.corr().abs()\n",
        "\n",
        "# # Get top 10 url features correlated with the target variable\n",
        "# top_correlated_third_party_columns = corr_matrix_third_party['y'].sort_values(ascending=False).head(10)\n",
        "# print(top_correlated_third_party_columns)"
      ],
      "metadata": {
        "id": "LL45aeHhHWkp",
        "outputId": "95287d6d-38d2-4c56-ce8b-4e31de307394",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y                             1.000000\n",
            "google_index                  0.738908\n",
            "page_rank                     0.503734\n",
            "domain_age                    0.325361\n",
            "domain_registration_length    0.170025\n",
            "dns_record                    0.111166\n",
            "whois_registered_domain       0.074040\n",
            "web_traffic                   0.064179\n",
            "Name: y, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Boxplots - TODO - fix or eliminate\n",
        "\n",
        "A quick (if bulky) look at the distributions the features.  There appear to be some invalid values, mostly -1s in features where these values don't make sense."
      ],
      "metadata": {
        "id": "hk7v6S5KFl_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Boxplots\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the number of columns per plot\n",
        "columns_per_plot = 12\n",
        "\n",
        "# Split the numeric columns into chunks\n",
        "chunks = [Xy_train[i:i + columns_per_plot] for i in range(0, len(Xy_train), columns_per_plot)]\n",
        "\n",
        "# Create boxplots for each chunk\n",
        "for i, chunk in enumerate(chunks):\n",
        "    plt.figure()\n",
        "    Xy_train[chunk].boxplot()\n",
        "    plt.title(f'Boxplots for Columns {i * columns_per_plot + 1} to {(i + 1) * columns_per_plot}')\n",
        "    plt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "DkWlpJiNUB5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Parallel coordinate plots (correlated feature groups)\n",
        "\n",
        "I generated several groups of correlated features and plotted those groups (with the target variable) in parallel coordinate plots.\n",
        "\n",
        "##### **Narrowly-applicable Red Flags and Weak Indicators**\n",
        "\n",
        "This exposed interesting relationships that couldn't be seen in the correlation matrix.  For example, tld_in_subdomain = 1 in only about 5% of observations, but the vast majority of these are phishing.  In a similar vein, ip = 1 accounts for about 15% of observations, of which about 5/6 are phishing.  Each of these could potentially serve as a red flag for phishing, even if they don't provide much information about most of the dataset.   \n",
        "\n",
        "We can also see somewhat less narrowly-applicable features.  For example, setting a threshold of ratio_digits_url >= 0.05 gives us a little over 30% of observations, and of these almost 3/4 are phishing.  At nb_dots >= 4, we select about 13% of observations, nearly all phishing, but at nb_dots >= 3, this increases to about 33% of observations, which which almost 2/3 are phishing.\n",
        "\n",
        "This pattern continues across numerous features: they may serve as red flags at higher thresholds, or may provide useful correlation at lower thresholds, or in some cases they may do both.\n",
        "\n",
        "##### **Strong, Broadly-Applicable Indicators**\n",
        "\n",
        "As the correlation matrix indicated, google_index is a strong foundation for a phishing model by itself. page_rank and domain_age are also very strong, common indicators.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pEASoVxF1HzQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Group correlated features"
      ],
      "metadata": {
        "id": "tLunKbmUgrWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "#Create empty dataframes\n",
        "dfs_eda = []\n",
        "for i in range(0,13):\n",
        "  df = pd.DataFrame()\n",
        "  dfs_eda.append(df)\n",
        "\n",
        "form_group_threshold = 0.3\n",
        "join_group_threshold = 0.15\n",
        "\n",
        "\n",
        "# Add the target variable to the first data frame to ensure\n",
        "# highly-correlated variables will be explored\n",
        "dfs_eda[0]['y'] = y_train\n",
        "\n",
        "# Iterate through columns prioritizing those most correlated with the target\n",
        "# grouping them based on correlation with columns already assigned to groups,\n",
        "# correlation with the most-correlated other column, the presence of empty\n",
        "# groups, and correlation thresholds.\n",
        "for col in Xy_train:\n",
        "\n",
        "  #Get the next most-correlated column other than col itself\n",
        "\n",
        "  most_correlated = corr_matrix[col].sort_values(ascending=False).index[1]\n",
        "  correlation = corr_matrix.loc[col, most_correlated]\n",
        "\n",
        "  # Check if there are any remaining empty groups\n",
        "  empty_remaining = False\n",
        "  for df in dfs_eda:\n",
        "    if df.empty:\n",
        "      empty_remaining = True\n",
        "\n",
        "  # If there are empty remaining groups and correlation for the current column\n",
        "  # exceeds the threshold for group formation, add both to the first empty df\n",
        "  if empty_remaining and correlation > form_group_threshold:\n",
        "    for df in dfs_eda:\n",
        "      if df.empty:\n",
        "        df[col] = Xy_train[col]\n",
        "        df[most_correlated] = Xy_train[most_correlated]\n",
        "        break\n",
        "  # Otherwise, if the correlation exceeds the minimum threshold for joining a\n",
        "  # group\n",
        "  elif corr_matrix['y'][col] > 0.3:\n",
        "    dfs_eda[0][col] = Xy_train[col]\n",
        "  elif correlation > join_group_threshold:\n",
        "    for df in dfs_eda:\n",
        "      if most_correlated in df.columns:\n",
        "        df[col] = Xy_train[col]\n",
        "        break\n",
        "  elif corr_matrix['y'][col] > 0.15:\n",
        "    dfs_eda[0][col] = Xy_train[col]\n",
        "\n",
        "\n",
        "# Add the target variable to each group\n",
        "for df in dfs_eda:\n",
        "  if not 'y' in df.columns:\n",
        "    df['y'] = y_train\n",
        "\n"
      ],
      "metadata": {
        "id": "f-mxEeLymUc2"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for df in dfs_eda:\n",
        "  if not df.empty:\n",
        "    print(df.columns)"
      ],
      "metadata": {
        "id": "6MrdIqNvSHlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Generate parallel coordinate plots"
      ],
      "metadata": {
        "id": "MnwxuOD-guLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hiplot as hip\n",
        "\n",
        "# add y to df1\n",
        "df['y'] = y_train\n",
        "\n",
        "# convert dfs_eda to list of dicts because hiplot requires\n",
        "dicts_eda = [df.to_dict('records') for df in dfs_eda]\n",
        "\n",
        "for d in dicts_eda:\n",
        "  hip.Experiment.from_iterable(d).display()"
      ],
      "metadata": {
        "id": "xgHCZGmcVPyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Baseline Models"
      ],
      "metadata": {
        "id": "02JS_WFUUavy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Logistic Regression"
      ],
      "metadata": {
        "id": "cs8IoTZHHce0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set up pipeline\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "pipe_lr = make_pipeline(\n",
        "      StandardScaler(),\n",
        "      LogisticRegression(random_state=42)\n",
        ")\n",
        "\n",
        "pipe_lr.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "n-vSb9zqHbyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_model_evaluation(\"Logistic Regression\\nPerformance on Training Set\",\n",
        "                       pipe_lr, X_train, y_train)\n",
        "\n",
        "print_model_evaluation(\"Logistic Regression\\nPerformance on Validation Set\",\n",
        "                       pipe_lr, X_validation, y_validation)"
      ],
      "metadata": {
        "id": "Or1lEMszW3u9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "import pandas as pd\n",
        "\n",
        "# Define model evaluation function\n",
        "\n",
        "def print_model_evaluation(\n",
        "    title: str,\n",
        "    pipe : Pipeline,\n",
        "    X : pd.DataFrame,\n",
        "    y : pd.DataFrame):\n",
        "\n",
        "    print(\"\\n\" + title)\n",
        "    pred_y = pipe.predict(X)\n",
        "    print(confusion_matrix(pred_y, y))\n",
        "    print(\"\\nROC-AUC: \" + str(roc_auc_score(pred_y, y)))\n",
        "    print(\"Precision: \" + str(precision_score(pred_y, y)))\n",
        "    print(\"Recall: \" + str(recall_score(pred_y, y)))"
      ],
      "metadata": {
        "id": "UeRh1QGqHV6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Random Forest"
      ],
      "metadata": {
        "id": "2rPGTjm8W44U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set up & run pipeline - random forest\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "pipe_rf = make_pipeline(\n",
        "      StandardScaler(),\n",
        "      RandomForestClassifier(random_state=42)\n",
        ")\n",
        "\n",
        "pipe_rf.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "nDtdj6L7W4PZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_model_evaluation(\"Random Forest\\nPerformance on Training Set\",\n",
        "                       pipe_rf, X_train, y_train)\n",
        "\n",
        "print_model_evaluation(\"Random Forest\\nPerformance on Validation Set\",\n",
        "                       pipe_rf, X_validation, y_validation)\n"
      ],
      "metadata": {
        "id": "5vr4PsqKW5zA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feature Importance"
      ],
      "metadata": {
        "id": "IdxQc9wfHXXk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Gradient-boosted Trees"
      ],
      "metadata": {
        "id": "w-bFzlbyW4Ad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up and run pipeline - gradient boosted trees\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "pipe_gbt = make_pipeline(\n",
        "      StandardScaler(),\n",
        "      HistGradientBoostingClassifier(random_state=42)\n",
        ")\n",
        "\n",
        "pipe_gbt.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "TSMt4Oa8W6pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_model_evaluation(\"Gradient-boosted Trees\\nPerformance on Training Set\",\n",
        "                       pipe_gbt, X_train, y_train)\n",
        "\n",
        "print_model_evaluation(\"Gradient-boosted Trees\\nPerformance on Validation Set\",\n",
        "                       pipe_gbt, X_validation, y_validation)"
      ],
      "metadata": {
        "id": "rn-huRZLW6c4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####"
      ],
      "metadata": {
        "id": "EDxftzc3UVCp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Small Multiples -\n",
        "heatmaps for something?"
      ],
      "metadata": {
        "id": "ekeXb6Mivs1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Engineering and Selection"
      ],
      "metadata": {
        "id": "KC5iBGfQNT9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Development"
      ],
      "metadata": {
        "id": "XmBosQUuNhij"
      }
    }
  ]
}